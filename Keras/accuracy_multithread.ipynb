{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43b2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading accuracy_multithread.ipynb in format ipynb\n",
      "/home/ehsan/Partial_Q/Keras/env_Q_partial/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n",
      "[jupytext] Updating notebook metadata with '{\"jupytext\": {\"formats\": \"ipynb,py\"}}'\n",
      "[jupytext] Updating accuracy_multithread.ipynb\n",
      "[jupytext] Updating accuracy_multithread.py\n",
      "Requirement already satisfied: numpy in ./env_Q_partial/lib/python3.10/site-packages (1.26.1)\n",
      "Requirement already satisfied: tensorflow in ./env_Q_partial/lib/python3.10/site-packages (2.14.0)\n",
      "Requirement already satisfied: keras in ./env_Q_partial/lib/python3.10/site-packages (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (1.26.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (4.25.0)\n",
      "Requirement already satisfied: setuptools in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./env_Q_partial/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./env_Q_partial/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in ./env_Q_partial/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env_Q_partial/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./env_Q_partial/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./env_Q_partial/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./env_Q_partial/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env_Q_partial/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./env_Q_partial/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env_Q_partial/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_Q_partial/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env_Q_partial/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_Q_partial/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env_Q_partial/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./env_Q_partial/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./env_Q_partial/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env_Q_partial/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: Pillow in ./env_Q_partial/lib/python3.10/site-packages (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "!jupytext --set-formats ipynb,py accuracy_multithread.ipynb --sync\n",
    "!pip install numpy\n",
    "!pip install tensorflow keras\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5065427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ehsan/Partial_Q/Keras/env_Q_partial/bin/python\n"
     ]
    }
   ],
   "source": [
    "#make sure that it is the right environment\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b03e604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 08:41:55.516288: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-09 08:41:55.556781: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-09 08:41:55.556837: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-09 08:41:55.556863: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-09 08:41:55.563655: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-09 08:41:55.564504: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-09 08:41:56.567770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4b154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e977db",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "model_name='MobileNet/MobileNet.h5'\n",
    "models_dir='/home/ehsan/Partial_Q/models/'\n",
    "Proj_DIR=\"/home/ehsan/Partial_Q/\"\n",
    "IMAGE_DIR = Proj_DIR+'/Imagenet/ILSVRC2012_img_val'\n",
    "LABEL_FILE = Proj_DIR+'/Evaluation/Ground_labels/ground_labels.txt'\n",
    "LABEL_MAP=Proj_DIR+'/Keras/labels.txt'\n",
    "BATCH_SIZE = 100  # Adjust as needed\n",
    "N=50000\n",
    "MAX_WORKERS=64\n",
    "NUM_CLASSES = 1000  # Number of ImageNet classes\n",
    "\n",
    "_model_input_shape= (224, 224)\n",
    "\n",
    "\n",
    "\n",
    "def _load_model(model_path,model_input_shape=_model_input_shape):\n",
    "    #global model,model_format\n",
    "    # support of tflite model\n",
    "    if model_path.endswith('.tflite'):\n",
    "        from tensorflow.lite.python import interpreter as interpreter_wrapper\n",
    "        model = interpreter_wrapper.Interpreter(model_path=model_path)\n",
    "        model.allocate_tensors()\n",
    "        model_format = 'TFLITE'\n",
    "\n",
    "        #Ehsan input shape correctness\n",
    "        input_details = model.get_input_details()\n",
    "        input_shape = input_details[0]['shape']\n",
    "        input_shape[1] = model_input_shape[0]\n",
    "        input_shape[2] = model_input_shape[1]\n",
    "        input_shape[0]=BATCH_SIZE\n",
    "        model.resize_tensor_input(0, input_shape)\n",
    "        #print(f'shape of input is: {input_shape}')\n",
    "        #model.allocate_tensors()\n",
    "        \n",
    "\n",
    "\n",
    "    # support of MNN model\n",
    "    elif model_path.endswith('.mnn'):\n",
    "        model = MNN.Interpreter(model_path)\n",
    "        model_format = 'MNN'\n",
    "\n",
    "    # support of TF 1.x frozen pb model\n",
    "    elif model_path.endswith('.pb'):\n",
    "        model = load_graph(model_path)\n",
    "        model_format = 'PB'\n",
    "\n",
    "    # support of ONNX model\n",
    "    elif model_path.endswith('.onnx'):\n",
    "        model = onnxruntime.InferenceSession(model_path)\n",
    "        model_format = 'ONNX'\n",
    "\n",
    "    # normal keras h5 model\n",
    "    elif model_path.endswith('.h5'):\n",
    "        #custom_object_dict = get_custom_objects()\n",
    "\n",
    "        model = load_model(model_path, compile=False)#, custom_objects=custom_object_dict)\n",
    "        model_format = 'H5'\n",
    "        K.set_learning_phase(0)\n",
    "    else:\n",
    "        raise ValueError('invalid model file')\n",
    "\n",
    "    return model,model_format\n",
    "\n",
    "#_load_model(\"Quantization/cases/(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0).tflite\",[224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3b536c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"img_paths=['/home/ehsan/Partial_Q/Imagenet/ILSVRC2012_img_val/ILSVRC2012_val_00021428.JPEG',\\n           '/home/ehsan/Partial_Q/Imagenet/ILSVRC2012_img_val/ILSVRC2012_val_00021428.JPEG',\\n          '/home/ehsan/Partial_Q/Imagenet/ILSVRC2012_img_val/ILSVRC2012_val_00021429.JPEG']\\nbatch_imgs = np.vstack([preprocess_image(img_path) for img_path in img_paths])\\nmobile_predict_tflite(batch_imgs)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_tflite(image,model):\n",
    "    model.allocate_tensors()\n",
    "    input_details = model.get_input_details()\n",
    "    output_details = model.get_output_details()\n",
    "\n",
    "    # check the type of the input tensor\n",
    "    #if input_details[0]['dtype'] == np.float32:\n",
    "        #floating_model = True\n",
    "\n",
    "    \n",
    "    #height = input_details[0]['shape'][1]\n",
    "    #width = input_details[0]['shape'][2]\n",
    "    #model_input_shape = (height, width)\n",
    "    #model_input_shape = (608,608)\n",
    "    #print(f'{input_details}\\nimage shape:{np.array(image).shape}, model input shape {model_input_shape}')\n",
    "    #input()\n",
    "    #image_data = preprocess_image(image, model_input_shape)\n",
    "    #origin image shape, in (height, width) format\n",
    "    #image_shape = image.size[::-1]\n",
    "\n",
    "    #interpreter.set_tensor(input_details[0]['index'], image_data)\n",
    "    model.set_tensor(input_details[0]['index'], image)\n",
    "    model.invoke()\n",
    "\n",
    "    '''prediction = []\n",
    "    for output_detail in output_details:\n",
    "        output_data = model.get_tensor(output_detail['index'])\n",
    "        prediction.append(output_data)\n",
    "    return np.array(prediction[0]) '''  \n",
    "    outp=(model.get_tensor(output_details[0]['index'])).copy()\n",
    "    return outp\n",
    "        \n",
    "    #return model.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "\n",
    "'''img_paths=['/home/ehsan/Partial_Q/Imagenet/ILSVRC2012_img_val/ILSVRC2012_val_00021428.JPEG',\n",
    "           '/home/ehsan/Partial_Q/Imagenet/ILSVRC2012_img_val/ILSVRC2012_val_00021428.JPEG',\n",
    "          '/home/ehsan/Partial_Q/Imagenet/ILSVRC2012_img_val/ILSVRC2012_val_00021429.JPEG']\n",
    "batch_imgs = np.vstack([preprocess_image(img_path) for img_path in img_paths])\n",
    "mobile_predict_tflite(batch_imgs)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30252dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Load model\n",
    "def _load_model(MODEL_NAME):\n",
    "    #model = load_model(models_dir+model_name)\n",
    "    global model\n",
    "    model = load_model(MODEL_NAME)\n",
    "'''\n",
    "\n",
    "\n",
    "# Step 1: Create a mapping from class names to indices based on labels.txt\n",
    "label_index_map = {}\n",
    "with open(LABEL_MAP, 'r') as f:\n",
    "    for index, line in enumerate(f):\n",
    "        class_name = line.strip().split(' ')[0]\n",
    "        label_index_map[class_name] = index\n",
    "\n",
    "# Step 2: Read ground_labels.txt and convert the labels to indices\n",
    "ground_truth_indices = []\n",
    "with open(LABEL_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        class_name = line.strip().split(' ')[0]\n",
    "        if class_name in label_index_map:\n",
    "            ground_truth_indices.append(label_index_map[class_name])\n",
    "        else:\n",
    "            print(f\"Label {class_name} not found in label index map.\")\n",
    "            ground_truth_indices.append(None)  # Handle missing labels if necessary\n",
    "\n",
    "# Now, ground_truth_indices contains the true indices for each image\n",
    "# Preprocessing function\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def run_predict(batch_images, model, model_format):\n",
    "    if model_format=='H5':\n",
    "        batch_preds = model.predict(batch_images)\n",
    "    elif model_format=='TFLITE':\n",
    "        batch_preds = predict_tflite(batch_images, model)\n",
    "    return batch_preds\n",
    "\n",
    "\n",
    "# Evaluation function to be used in each process\n",
    "def evaluate_batch(batch_images, batch_labels, _m, m_form):\n",
    "    #global batch_preds\n",
    "    batch_preds = run_predict(batch_images, _m, m_form)\n",
    "    #print(batch_preds)\n",
    "    top1_correct = np.sum(np.argmax(batch_preds, axis=1) == batch_labels)\n",
    "    top5_correct = np.sum([label in pred for label, pred in zip(batch_labels, np.argsort(batch_preds, axis=1)[:, -5:])])\n",
    "    return top1_correct, top5_correct\n",
    "\n",
    "\n",
    "# Thread worker function\n",
    "def thread_worker(image_paths, labels, Model_Name):\n",
    "    #print(f'running for images {len(image_paths)}')\n",
    "    _m,m_form=_load_model(Model_Name)\n",
    "    batch_images = np.vstack([preprocess_image(img_path) for img_path in image_paths])\n",
    "    top1 , top5 = evaluate_batch(batch_images, labels, _m, m_form)\n",
    "    del _m\n",
    "    return [top1, top5]\n",
    "\n",
    "\n",
    "\n",
    "def main(Model_Name=models_dir+model_name):\n",
    "    #_load_model(Model_Name)\n",
    "    # Gather image paths and labels\n",
    "    image_paths = [os.path.join(IMAGE_DIR, fname) for fname in sorted(os.listdir(IMAGE_DIR))][:N]\n",
    "\n",
    "    #labels = to_categorical(true_labels, NUM_CLASSES)\n",
    "    labels=ground_truth_indices\n",
    "\n",
    "    # Split into batches\n",
    "    batches = [(image_paths[i:i + BATCH_SIZE], labels[i:i + BATCH_SIZE]) for i in range(0, len(image_paths), BATCH_SIZE)]\n",
    "\n",
    "\n",
    "    time1=time.time()\n",
    "    # Perform multi-threaded evaluation\n",
    "    top1_correct = top5_correct = total_images = 0\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all the tasks and get back Future objects\n",
    "        futures = [executor.submit(thread_worker, batch[0], batch[1],Model_Name) for batch in batches]\n",
    "\n",
    "        # Iterate over the completed futures as they complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            top1, top5 = future.result()  # Unpack the result from the future\n",
    "            top1_correct += top1\n",
    "            top5_correct += top5\n",
    "            total_images += BATCH_SIZE  # Make sure to update the total_images if not all images are used\n",
    "\n",
    "    time2=time.time()\n",
    "    t=time2-time1\n",
    "    print(f\"Total time of Evaluation: {t}\")\n",
    "    # Calculate overall accuracies\n",
    "    overall_top1_accuracy = top1_correct / total_images  # This should be total_images, not N, in case N is not a multiple of BATCH_SIZE\n",
    "    overall_top5_accuracy = top5_correct / total_images\n",
    "    print(f\"Overall top-1 accuracy: {overall_top1_accuracy * 100:.4f}%\")\n",
    "    print(f\"Overall top-5 accuracy: {overall_top5_accuracy * 100:.4f}%\")\n",
    "    return overall_top1_accuracy,overall_top5_accuracy\n",
    "     \n",
    "\n",
    "#main()\n",
    "#main('/home/ehsan/Partial_Q/Keras/Quantization/cases/(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0).tflite')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Q_Partial",
   "language": "python",
   "name": "q_partial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
